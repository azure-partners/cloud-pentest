import json
import dns.resolver
import sublist3r
import ipaddress
from termcolor import cprint
import argparse
import csv
import webbrowser
from providers.azure import get_azure_networks
from providers.gcp import get_gcp_networks
from providers.aws import get_aws_networks


def read_subdomains(filename):
    with open(filename, 'r') as txt_file:
        return txt_file.readlines()

def find_subdomains(domain, verbose):
    return sublist3r.main(domain, 40, None, ports=None, silent=not verbose, verbose=verbose, enable_bruteforce=False, engines=None)

def read_networks(filename):
    with open(filename, 'r') as networks_file:
        networks_str = json.load(networks_file)
    return {ipaddress.ip_network(k): v for k,v in networks_str.items()}

def in_network(ip: ipaddress.ip_address, networks: dict[ipaddress.ip_network, str]):
    netmask = 0xFFFFFFFF
    bits = 32
    while netmask != 0:
        network_option = ipaddress.ip_network((int(ip)&netmask, bits))
        if network_option in networks:
            return networks[network_option]
        netmask=(netmask << 1) & 0xFFFFFFFF
        bits-=1
    return None

def export_csv(filename, subdomains, networks, openlinks):
    print_dbg(f"[-] Exporting findings to {filename}", 'blue')
    with open(f'{filename}', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Subdomain", "IP Address", "Service Tag"])
        for subdomain in subdomains:
            try:
                subdomain = subdomain.rstrip()
                ips = list(map(str, dns.resolver.resolve(subdomain)))
                service_tag = in_network(ipaddress.ip_address(ips[0]), networks)
                print_dbg(f"{subdomain:<70}| {ips[0]:<15}| {service_tag}", 'green')
                writer.writerow([subdomain, ', '.join(ips), service_tag])
                if openlinks:
                    webbrowser.open(subdomain)
            except dns.exception.DNSException:
                print_dbg(f"{subdomain:<70}| Not Found      | None", 'red')

def main():
    global print_dbg
    parser = argparse.ArgumentParser(description='Pentest Tool to find dangerous subdomains in the Cloud network')
    parser.add_argument('-v', '--verbose', action="store_true", help="increase output verbosity")
    parser.add_argument('-o', '--openlinks', action="store_true", help="open links in the browsers")
    group0 = parser.add_mutually_exclusive_group(required=True)
    group0.add_argument('-g', '--google',    action='store_true', help='check google cloud (gcp)')
    group0.add_argument('-a', '--amazon',    action='store_true', help='check amazon cloud (aws)')
    group0.add_argument('-m', '--microsoft', action='store_true', help='check microsoft cloud (azure)')
    group1 = parser.add_mutually_exclusive_group(required=True)
    group1.add_argument('-d', '--domain', type=str, help='specify the domain to check')
    group1.add_argument('-df', '--domainfile', type=str, help='use a text file with a list of addresses to check')
    group2 = parser.add_mutually_exclusive_group(required=True)
    group2.add_argument('-w', '--whitelist', action='store_true', help='get networks whitelist from cloud api')
    group2.add_argument('-wf', '--whitelistfile', type=str, help='read networks from json file')
    args = parser.parse_args()
    
    print_dbg = lambda *x: cprint(*x) if args.verbose else None

    get_networks = get_gcp_networks if args.google else get_aws_networks if args.amazon else get_azure_networks
    networks = read_networks(args.whitelistfile) if args.whitelistfile else get_networks(args.verbose)
    subdomains = read_subdomains(args.domainfile) if args.domainfile else find_subdomains(args.domain, args.verbose)
    name = args.domainfile if args.domainfile else args.domain

    export_csv(f"csv/{name}.csv", subdomains, networks, args.openlinks)

if __name__ == "__main__":
    main()